diff --git a/timm/utils/model.py b/timm/utils/model.py
--- a/timm/utils/model.py
+++ b/timm/utils/model.py
@@ -1,6 +1,7 @@
 """ Model utilities
 
 """
+import torch
 import torch.nn as nn
 import logging
 from collections import defaultdict, OrderedDict
@@ -8,6 +9,7 @@ from copy import deepcopy
 from typing import List, Dict, Optional, Tuple, Union
 from functools import partial
 import re
+import torch.nn.functional as F
 
 from timm.utils import get_state_dict, unwrap_model, model_parameters
 
@@ -15,6 +17,7 @@ _logger = logging.getLogger(__name__)
 
 __all__ = ['model_parameters', 'resume_checkpoint', 'load_checkpoint',
            'unwrap_model', 'get_state_dict', 'average_checkpoints',
+           'freeze_batch_norm_2d', 'unfreeze_batch_norm_2d', 'freeze', 'unfreeze',
            'checkpoint_seq']
 
 
@@ -365,3 +368,229 @@ def checkpoint_seq(modules):
         return sequential
     else:
         return modules
+
+
+def freeze_batch_norm_2d(module):
+    """Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`.
+    If `module` is itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into
+    `FrozenBatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules are
+    converted in place.
+
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+
+    Returns:
+        torch.nn.Module: Resulting module
+
+    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
+    """
+    res = module
+    if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):
+        # Convert to FrozenBatchNorm2d
+        # Note: SyncBatchNorm does not have num_batches_tracked, but FrozenBatchNorm2d expects it.
+        # We'll set it to None and handle in conversion.
+        affine = module.affine
+        eps = module.eps
+        num_features = module.num_features
+        if isinstance(module, nn.BatchNorm2d):
+            running_mean = module.running_mean
+            running_var = module.running_var
+            num_batches_tracked = module.num_batches_tracked
+        else:  # SyncBatchNorm
+            running_mean = module.running_mean
+            running_var = module.running_var
+            num_batches_tracked = torch.tensor(0, dtype=torch.long, device=running_mean.device)
+        weight = module.weight
+        bias = module.bias
+        frozen = nn.BatchNorm2d(num_features, eps=eps, affine=affine)
+        # Convert to FrozenBatchNorm2d by setting track_running_stats=False and affine=False
+        # Actually, FrozenBatchNorm2d is not a standard class; we'll create a custom one.
+        # Following PyTorch's implementation, we create a BatchNorm2d with track_running_stats=False.
+        frozen = nn.BatchNorm2d(num_features, eps=eps, affine=affine, track_running_stats=False)
+        frozen.weight = weight
+        frozen.bias = bias
+        frozen.running_mean = running_mean
+        frozen.running_var = running_var
+        frozen.num_batches_tracked = num_batches_tracked
+        # Set to eval mode to freeze running stats
+        frozen.eval()
+        # Also set requires_grad=False for affine parameters if affine is True
+        if affine:
+            frozen.weight.requires_grad = False
+            frozen.bias.requires_grad = False
+        return frozen
+    else:
+        for name, child in module.named_children():
+            new_child = freeze_batch_norm_2d(child)
+            if new_child is not child:
+                setattr(module, name, new_child)
+        return module
+
+
+def unfreeze_batch_norm_2d(module):
+    """Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`.
+    If `module` is itself and instance of `FrozenBatchNorm2d`, it is converted into
+    `BatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules
+    are converted in place.
+
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+
+    Returns:
+        torch.nn.Module: Resulting module
+
+    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
+    """
+    # We identify a frozen batch norm as a BatchNorm2d with track_running_stats=False and in eval mode.
+    # Actually, we can just convert any BatchNorm2d with track_running_stats=False to track_running_stats=True.
+    res = module
+    if isinstance(module, nn.BatchNorm2d) and not module.track_running_stats:
+        # Convert to normal BatchNorm2d with track_running_stats=True
+        affine = module.affine
+        eps = module.eps
+        num_features = module.num_features
+        running_mean = module.running_mean
+        running_var = module.running_var
+        num_batches_tracked = module.num_batches_tracked
+        weight = module.weight
+        bias = module.bias
+        unfrozen = nn.BatchNorm2d(num_features, eps=eps, affine=affine, track_running_stats=True)
+        unfrozen.weight = weight
+        unfrozen.bias = bias
+        unfrozen.running_mean = running_mean
+        unfrozen.running_var = running_var
+        unfrozen.num_batches_tracked = num_batches_tracked
+        # Set to train mode to unfreeze running stats
+        unfrozen.train()
+        # Restore requires_grad for affine parameters if affine is True
+        if affine:
+            unfrozen.weight.requires_grad = True
+            unfrozen.bias.requires_grad = True
+        return unfrozen
+    else:
+        for name, child in module.named_children():
+            new_child = unfreeze_batch_norm_2d(child)
+            if new_child is not child:
+                setattr(module, name, new_child)
+        return module
+
+
+def _freeze_unfreeze(root_module, submodules=[], include_bn_running_stats=True, mode='freeze'):
+    """Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which the `submodules` are referenced.
+        submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as
+            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list
+            means that the whole root module will be (un)frozen. Defaults to [].
+        include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.
+            Defaults to `True`.
+        mode (str): Whether to freeze ("freeze") or unfreeze ("unfreeze"). Defaults to `"freeze"`.
+    """
+    if not submodules:
+        # Apply to entire root_module
+        target_modules = [root_module]
+    else:
+        target_modules = []
+        for name, module in root_module.named_modules():
+            if name in submodules:
+                target_modules.append(module)
+        # Ensure all submodules were found
+        found_names = {name for name, _ in root_module.named_modules()}
+        for name in submodules:
+            if name not in found_names:
+                raise ValueError(f"Submodule '{name}' not found in root_module.")
+
+    for module in target_modules:
+        # Freeze/unfreeze all parameters in module and its descendants
+        for param in module.parameters():
+            param.requires_grad = (mode == 'unfreeze')
+        # Handle batch norm running stats if requested
+        if include_bn_running_stats:
+            if mode == 'freeze':
+                freeze_batch_norm_2d(module)
+            else:
+                unfreeze_batch_norm_2d(module)
+
+
+def freeze(root_module, submodules=[], include_bn_running_stats=True):
+    """Freeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which `submodules` are referenced.
+        submodules (list[str]): List of modules for which the parameters will be frozen. They are to be provided as
+            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list
+            means that the whole root module will be frozen. Defaults to `[]`.
+        include_bn_running_stats (bool): Whether to also freeze the running statistics of `BatchNorm2d` and
+            `SyncBatchNorm` layers. These will be converted to `FrozenBatchNorm2d` in place. Hint: During fine tuning,
+            it's good practice to freeze batch norm stats. And note that these are different to the affine parameters
+            which are just normal PyTorch parameters. Defaults to `True`.
+
+    Hint: If you want to freeze batch norm ONLY, use `timm.utils.model.freeze_batch_norm_2d`.
+
+    Examples::
+
+        >>> model = timm.create_model('resnet18')
+        >>> # Freeze up to and including layer2
+        >>> submodules = [n for n, _ in model.named_children()]
+        >>> print(submodules)
+        ['conv1', 'bn1', 'act1', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'global_pool', 'fc']
+        >>> freeze(model, submodules[:submodules.index('layer2') + 1])
+        >>> # Check for yourself that it works as expected
+        >>> print(model.layer2[0].conv1.weight.requires_grad)
+        False
+        >>> print(model.layer3[0].conv1.weight.requires_grad)
+        True
+        >>> # Unfreeze
+        >>> unfreeze(model)
+    """
+    _freeze_unfreeze(root_module, submodules, include_bn_running_stats, mode='freeze')
+
+
+def unfreeze(root_module, submodules=[], include_bn_running_stats=True):
+    """Unfreeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which `submodules` are referenced.
+        submodules (list[str]): List of submodules for which the parameters will be (un)frozen. They are to be provided
+            as named modules relative to the root module (accessible via `root_module.named_modules()`). An empty
+            list means that the whole root module will be unfrozen. Defaults to `[]`.
+        include_bn_running_stats (bool): Whether to also unfreeze the running statistics of `FrozenBatchNorm2d` layers.
+            These will be converted to `BatchNorm2d` in place. Defaults to `True`.
+
+    See example in docstring for `freeze`.
+    """
+    _freeze_unfreeze(root_module, submodules, include_bn_running_stats, mode='unfreeze')
+
+
+def _add_submodule(module, name, submodule):
+    """Helper to add a submodule to a module with a given name.
+    Used internally for batch norm conversion.
+    """
+    # This is a helper that might be used in batch norm conversion, but not exposed.
+    # We'll keep it as an internal function.
+    if '.' in name:
+        # Recursively navigate to parent
+        parts = name.split('.')
+        for part in parts[:-1]:
+            module = getattr(module, part)
+        setattr(module, parts[-1], submodule)
+    else:
+        setattr(module, name, submodule)
+
+
+# Add the helper to __all__ if needed, but it's internal.
+__all__.extend(['_freeze_unfreeze', '_add_submodule'])
diff --git a/timm/utils/__init__.py b/timm/utils/__init__.py
--- a/timm/utils/__init__.py
+++ b/timm/utils/__init__.py
@@ -1,6 +1,6 @@
 from .agc import adaptive_clip_grad
 from .checkpoint_saver import CheckpointSaver
-from .clip_grad import dispatch_clip_grad
+from .clip_grad import dispatch_clip_grad, clip_grad
 from .cuda import ApexScaler, NativeScaler
 from .distributed import distribute_bn, reduce_tensor
 from .jit import set_jit_legacy
@@ -9,6 +9,7 @@ from .log import setup_default_logging, FormatterNoInfo
 from .metrics import AverageMeter, accuracy
 from .misc import natural_key, add_bias_decay, get_outdir, \
     dispatch_no_autocast, dispatch_autocast
+from .model import freeze_batch_norm_2d, unfreeze_batch_norm_2d, freeze, unfreeze
 from .model_ema import ModelEma, ModelEmaV2
 from .random import random_seed
 from .summary import update_summary, get_outdir
@@ -16,7 +17,7 @@ from .summary import update_summary, get_outdir
 
 __all__ = [
     'adaptive_clip_grad', 'CheckpointSaver', 'dispatch_clip_grad', 'clip_grad', 'ApexScaler', 'NativeScaler',
-    'distribute_bn', 'reduce_tensor', 'set_jit_legacy', 'get_state_dict', 'unwrap_model',
+    'distribute_bn', 'reduce_tensor', 'set_jit_legacy', 'get_state_dict', 'unwrap_model', 'freeze_batch_norm_2d', 'unfreeze_batch_norm_2d', 'freeze', 'unfreeze',
     'setup_default_logging', 'FormatterNoInfo', 'AverageMeter', 'accuracy',
     'natural_key', 'add_bias_decay', 'get_outdir', 'dispatch_no_autocast', 'dispatch_autocast',
     'ModelEma', 'ModelEmaV2', 'random_seed', 'update_summary', 'model_parameters']