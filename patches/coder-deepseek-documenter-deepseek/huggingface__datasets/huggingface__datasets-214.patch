diff --git a/src/nlp/arrow_dataset.py b/src/nlp/arrow_dataset.py
--- a/src/nlp/arrow_dataset.py
+++ b/src/nlp/arrow_dataset.py
@@ -1,5 +1,6 @@
 import copy
 import os
+import pyarrow as pa
 from dataclasses import dataclass
 from typing import Any, Callable, Dict, List, Optional, Union
 
@@ -1000,6 +1001,262 @@ class Dataset(DatasetMixin):
         )
         return self._new_dataset_with_indices(indices)
 
+    def filter(
+        self,
+        function: Optional[Callable] = None,
+        with_indices: bool = False,
+        input_columns: Optional[Union[str, List[str]]] = None,
+        batched: bool = False,
+        batch_size: Optional[int] = 1000,
+        remove_columns: Optional[List[str]] = None,
+        keep_in_memory: bool = False,
+        load_from_cache_file: bool = True,
+        cache_file_name: Optional[str] = None,
+        writer_batch_size: Optional[int] = 1000,
+        disable_nullable: bool = True,
+        fn_kwargs: Optional[dict] = None,
+        num_proc: Optional[int] = None,
+        desc: Optional[str] = None,
+    ) -> "Dataset":
+        """
+        Apply a filter function to all elements in the dataset and return a new dataset
+        containing only the examples for which the function returns True.
+        
+        Args:
+            function (`callable`): Filter function with one of the following signatures:
+                - `function(example: Dict) -> bool` if `with_indices=False` and `batched=False`
+                - `function(example: Dict, indices: int) -> bool` if `with_indices=True` and `batched=False`
+                - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False`
+                - `function(batch: Dict[str, List], indices: List[int]) -> List[bool]` if `batched=True` and `with_indices=True`
+            with_indices (`bool`, defaults to `False`): Provide example indices to `function`.
+            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): Columns to be passed to `function` as positional arguments.
+            batched (`bool`, defaults to `False`): Provide batches of examples to `function`.
+            batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`.
+            remove_columns (`Optional[List[str]]`, defaults to `None`): Remove columns before filtering.
+            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing to cache.
+            load_from_cache_file (`bool`, defaults to `True`): Use cache file if available.
+            cache_file_name (`Optional[str]`, defaults to `None`): Name of cache file to use.
+            writer_batch_size (`Optional[int]`, defaults to `1000`): Number of rows per write operation.
+            disable_nullable (`bool`, defaults to `True`): Allow null values in the table.
+            fn_kwargs (`Optional[dict]`, defaults to `None`): Additional keyword arguments to pass to `function`.
+            num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing.
+            desc (`Optional[str]`, defaults to `None`): Description for tqdm progress bar.
+        
+        Returns:
+            `Dataset`: A new dataset with filtered examples.
+        """
+        # Input validation
+        if function is None:
+            raise ValueError("Filter function cannot be None")
+        
+        # Prepare kwargs for internal implementation
+        filter_kwargs = {
+            'with_indices': with_indices,
+            'input_columns': input_columns,
+            'batched': batched,
+            'batch_size': batch_size,
+            'remove_columns': remove_columns,
+            'keep_in_memory': keep_in_memory,
+            'load_from_cache_file': load_from_cache_file,
+            'cache_file_name': cache_file_name,
+            'writer_batch_size': writer_batch_size,
+            'disable_nullable': disable_nullable,
+            'fn_kwargs': fn_kwargs,
+            'num_proc': num_proc,
+            'desc': desc or "Filtering",
+        }
+        
+        # Choose implementation based on dataset size and batched flag
+        if batched or len(self) > 10000:
+            # Use Arrow implementation for better performance with large datasets or batched processing
+            return self._filter_with_arrow(function, **filter_kwargs)
+        else:
+            # Use map-based implementation for smaller datasets
+            return self._filter_implementation(function, **filter_kwargs)
+
+    def _filter_implementation(
+        self,
+        function: Callable,
+        with_indices: bool = False,
+        input_columns: Optional[Union[str, List[str]]] = None,
+        batched: bool = False,
+        batch_size: Optional[int] = 1000,
+        **kwargs,
+    ) -> "Dataset":
+        """
+        Internal implementation of the filter operation using map.
+        """
+        # Create a wrapper function that converts filter results to the format expected by map
+        def filter_wrapper(*args, **wrapper_kwargs):
+            if batched:
+                # For batched processing
+                if with_indices:
+                    batch, indices = args[0], args[1]
+                    filter_results = function(batch, indices, **wrapper_kwargs)
+                else:
+                    batch = args[0]
+                    filter_results = function(batch, **wrapper_kwargs)
+                
+                # Convert boolean list to indices of examples to keep
+                indices_to_keep = [i for i, keep in enumerate(filter_results) if keep]
+                
+                if indices_to_keep:
+                    # Filter the batch to only include kept examples
+                    filtered_batch = {}
+                    for key in batch:
+                        filtered_batch[key] = [batch[key][i] for i in indices_to_keep]
+                    return filtered_batch
+                else:
+                    # Return empty batch if no examples are kept
+                    return {key: [] for key in batch}
+            else:
+                # For single example processing
+                if with_indices:
+                    example, idx = args[0], args[1]
+                    keep = function(example, idx, **wrapper_kwargs)
+                else:
+                    example = args[0]
+                    keep = function(example, **wrapper_kwargs)
+                
+                # Return the example if it should be kept, None otherwise
+                return example if keep else None
+        
+        # Use map with the wrapper function
+        filtered_dataset = self.map(
+            filter_wrapper,
+            with_indices=with_indices,
+            input_columns=input_columns,
+            batched=batched,
+            batch_size=batch_size,
+            remove_columns=kwargs.get('remove_columns'),
+            keep_in_memory=kwargs.get('keep_in_memory', False),
+            load_from_cache_file=kwargs.get('load_from_cache_file', True),
+            cache_file_name=kwargs.get('cache_file_name'),
+            writer_batch_size=kwargs.get('writer_batch_size', 1000),
+            disable_nullable=kwargs.get('disable_nullable', True),
+            fn_kwargs=kwargs.get('fn_kwargs'),
+            num_proc=kwargs.get('num_proc'),
+            desc=kwargs.get('desc', "Filtering"),
+        )
+        
+        # Remove None values (filtered out examples)
+        return self._remove_filtered_examples(filtered_dataset)
+
+    def _remove_filtered_examples(self, dataset_with_nones: "Dataset") -> "Dataset":
+        """
+        Remove examples that were filtered out (returned as None).
+        """
+        # Get indices of non-None examples
+        valid_indices = []
+        for i in range(len(dataset_with_nones)):
+            # Check if the example is valid (not filtered out)
+            if self._is_valid_example(dataset_with_nones[i]):
+                valid_indices.append(i)
+        
+        # Create a new dataset with only valid examples
+        return dataset_with_nones.select(valid_indices)
+
+    def _is_valid_example(self, example: dict) -> bool:
+        """
+        Check if an example is valid (not filtered out).
+        """
+        return example is not None and len(example) > 0
+
+    def _filter_with_arrow(
+        self,
+        function: Callable,
+        with_indices: bool = False,
+        input_columns: Optional[Union[str, List[str]]] = None,
+        batched: bool = False,
+        batch_size: Optional[int] = 1000,
+        **kwargs,
+    ) -> "Dataset":
+        """
+        Filter implementation using direct Arrow table operations for better performance.
+        """
+        # Get the underlying Arrow table
+        table = self.data
+        
+        if batched:
+            # Process in batches
+            all_indices_to_keep = []
+            batch_size = batch_size or 1000
+            
+            for i in range(0, len(table), batch_size):
+                batch_end = min(i + batch_size, len(table))
+                batch_indices = list(range(i, batch_end))
+                
+                # Extract batch
+                batch = table.slice(i, batch_end - i)
+                batch_dict = batch.to_pydict()
+                
+                # Apply input_columns filtering if specified
+                if input_columns is not None:
+                    if isinstance(input_columns, str):
+                        input_columns = [input_columns]
+                    batch_dict = {col: batch_dict[col] for col in input_columns if col in batch_dict}
+                
+                if with_indices:
+                    filter_results = function(batch_dict, batch_indices, **kwargs.get('fn_kwargs', {}))
+                else:
+                    filter_results = function(batch_dict, **kwargs.get('fn_kwargs', {}))
+                
+                # Get indices to keep from this batch
+                batch_keep_indices = [batch_indices[j] for j, keep in enumerate(filter_results) if keep]
+                all_indices_to_keep.extend(batch_keep_indices)
+        else:
+            # Process individual examples
+            all_indices_to_keep = []
+            
+            for i in range(len(table)):
+                example = table.slice(i, 1).to_pydict()
+                # Convert single example from batch format
+                single_example = {k: v[0] for k, v in example.items()}
+                
+                # Apply input_columns filtering if specified
+                if input_columns is not None:
+                    if isinstance(input_columns, str):
+                        input_columns = [input_columns]
+                    single_example = {col: single_example[col] for col in input_columns if col in single_example}
+                
+                if with_indices:
+                    keep = function(single_example, i, **kwargs.get('fn_kwargs', {}))
+                else:
+                    keep = function(single_example, **kwargs.get('fn_kwargs', {}))
+                
+                if keep:
+                    all_indices_to_keep.append(i)
+        
+        # Create new dataset with filtered indices
+        return self.select(all_indices_to_keep)
+
     def flatten(self, max_depth=16):
         """
         Flatten the Table, i.e. the nested columns are flattened and their child fields are moved to the root.