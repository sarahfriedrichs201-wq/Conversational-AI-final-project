diff --git a/timm/utils/__init__.py b/timm/utils/__init__.py
--- a/timm/utils/__init__.py
+++ b/timm/utils/__init__.py
@@ -1,6 +1,6 @@
 from .agc import adaptive_clip_grad
 from .checkpoint_saver import CheckpointSaver
-from .clip_grad import dispatch_clip_grad, clip_grad
+from .clip_grad import dispatch_clip_grad, clip_grad
 from .cuda import ApexScaler, NativeScaler
 from .distributed import distribute_bn, reduce_tensor
 from .jit import set_jit_legacy
@@ -9,6 +9,7 @@
 from .metrics import AverageMeter, accuracy
 from .misc import natural_key, add_bool_arg
 from .model import (
+    freeze, unfreeze, freeze_batch_norm_2d, unfreeze_batch_norm_2d,
     get_state_dict, unwrap_model, model_parameters,
     resume_checkpoint, load_checkpoint, load_pretrained,
     convert_splitbn_model, convert_sync_batchnorm)
diff --git a/timm/utils/model.py b/timm/utils/model.py
--- a/timm/utils/model.py
+++ b/timm/utils/model.py
@@ -1,6 +1,7 @@
 """ Model / state_dict utils
 
 Hacked together by / Copyright 2020 Ross Wightman
+Layer freezing utilities added by / Copyright 2024
 """
 import logging
 import os
@@ -8,6 +9,7 @@
 from collections import OrderedDict
 from copy import deepcopy
 from typing import Any, Callable, Dict, List, Optional, Tuple
+import torch.nn.functional as F
 
 import torch
 import torch.nn as nn
@@ -15,6 +17,7 @@
 from .model_ema import ModelEma
 
 _logger = logging.getLogger(__name__)
+_FrozenBatchNorm2d = None  # defined below
 
 
 def get_state_dict(model: nn.Module):
@@ -549,3 +552,299 @@
         model.load_state_dict(state_dict, strict=strict)
     else:
         _logger.warning("No checkpoint found at %s", checkpoint_path)
+
+
+class _FrozenBatchNorm2d(torch.nn.Module):
+    """
+    BatchNorm2d where the batch statistics and the affine parameters are fixed.
+    
+    This is adapted from PyTorch's implementation with modifications for freezing.
+    """
+    
+    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):
+        super(_FrozenBatchNorm2d, self).__init__()
+        self.num_features = num_features
+        self.eps = eps
+        self.momentum = momentum
+        self.affine = affine
+        self.track_running_stats = track_running_stats
+        
+        if self.affine:
+            self.weight = torch.nn.Parameter(torch.ones(num_features))
+            self.bias = torch.nn.Parameter(torch.zeros(num_features))
+        else:
+            self.register_parameter('weight', None)
+            self.register_parameter('bias', None)
+            
+        if self.track_running_stats:
+            self.register_buffer('running_mean', torch.zeros(num_features))
+            self.register_buffer('running_var', torch.ones(num_features))
+            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
+        else:
+            self.register_parameter('running_mean', None)
+            self.register_parameter('running_var', None)
+            self.register_parameter('num_batches_tracked', None)
+        
+        # Freeze all parameters
+        for param in self.parameters():
+            param.requires_grad = False
+    
+    def forward(self, x):
+        # Move running stats to same device as input
+        if self.track_running_stats:
+            running_mean = self.running_mean
+            running_var = self.running_var
+        else:
+            running_mean = None
+            running_var = None
+            
+        return F.batch_norm(
+            x, running_mean, running_var, self.weight, self.bias,
+            False, 0.0, self.eps
+        )
+    
+    def __repr__(self):
+        return f'_FrozenBatchNorm2d({self.num_features}, eps={self.eps}, momentum={self.momentum}, affine={self.affine})'
+    
+    @classmethod
+    def convert_frozen_batchnorm(cls, module):
+        """
+        Convert BatchNorm2d and SyncBatchNorm layers to FrozenBatchNorm2d in-place.
+        """
+        bn_module = nn.modules.batchnorm
+        if isinstance(module, (bn_module.BatchNorm2d, bn_module.SyncBatchNorm)):
+            frozen_bn = cls(
+                num_features=module.num_features,
+                eps=module.eps,
+                momentum=module.momentum,
+                affine=module.affine,
+                track_running_stats=module.track_running_stats
+            )
+            
+            # Copy parameters
+            if module.affine:
+                frozen_bn.weight.data = module.weight.data.clone()
+                frozen_bn.bias.data = module.bias.data.clone()
+            
+            # Copy running stats
+            if module.track_running_stats:
+                frozen_bn.running_mean.data = module.running_mean.data.clone()
+                frozen_bn.running_var.data = module.running_var.data.clone()
+                frozen_bn.num_batches_tracked.data = module.num_batches_tracked.data.clone()
+            
+            return frozen_bn
+        
+        # Recursively convert
+        for name, child in module.named_children():
+            new_child = cls.convert_frozen_batchnorm(child)
+            if new_child is not child:
+                setattr(module, name, new_child)
+        
+        return module
+
+
+def freeze_batch_norm_2d(module):
+    """
+    Converts all BatchNorm2d and SyncBatchNorm layers of provided module into FrozenBatchNorm2d.
+    
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+    
+    Returns:
+        torch.nn.Module: Resulting module
+    """
+    import torch.nn as nn
+    
+    # Handle case where module itself is a batch norm layer
+    if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):
+        frozen_bn = _FrozenBatchNorm2d(module)
+        return frozen_bn
+    
+    # Recursively convert submodules
+    for name, child in module.named_children():
+        new_child = freeze_batch_norm_2d(child)
+        if new_child is not child:
+            setattr(module, name, new_child)
+    
+    return module
+
+
+def unfreeze_batch_norm_2d(module):
+    """
+    Converts all FrozenBatchNorm2d layers of provided module into BatchNorm2d.
+    
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+    
+    Returns:
+        torch.nn.Module: Resulting module
+    """
+    import torch.nn as nn
+    
+    # Handle case where module itself is a frozen batch norm
+    if isinstance(module, _FrozenBatchNorm2d):
+        regular_bn = nn.BatchNorm2d(
+            num_features=module.num_features,
+            eps=module.eps,
+            momentum=module.momentum,
+            affine=module.affine,
+            track_running_stats=module.track_running_stats
+        )
+        
+        # Copy parameters
+        if module.affine:
+            regular_bn.weight.data = module.weight.data.clone()
+            regular_bn.bias.data = module.bias.data.clone()
+        
+        # Copy running stats
+        if module.track_running_stats:
+            regular_bn.running_mean.data = module.running_mean.data.clone()
+            regular_bn.running_var.data = module.running_var.data.clone()
+            regular_bn.num_batches_tracked.data = module.num_batches_tracked.data.clone()
+        
+        return regular_bn
+    
+    # Recursively convert submodules
+    for name, child in module.named_children():
+        new_child = unfreeze_batch_norm_2d(child)
+        if new_child is not child:
+            setattr(module, name, new_child)
+    
+    return module
+
+
+def _get_parent_and_child_name(root_module, target_module):
+    """
+    Helper to find parent module name and child attribute name for a given module.
+    """
+    for parent_name, parent in root_module.named_modules():
+        for child_name, child in parent.named_children():
+            if child is target_module:
+                return parent_name, child_name
+    return None, None
+
+
+def _freeze_unfreeze(root_module, submodules=[], include_bn_running_stats=True, mode='freeze'):
+    """
+    Internal helper function to freeze or unfreeze parameters.
+    
+    Args:
+        root_module (nn.Module): Root module relative to which submodules are referenced.
+        submodules (list[str]): List of module names to (un)freeze.
+        include_bn_running_stats (bool): Whether to handle batch norm running stats.
+        mode (str): 'freeze' or 'unfreeze'.
+    """
+    import torch.nn as nn
+    
+    # Validate mode
+    if mode not in ['freeze', 'unfreeze']:
+        raise ValueError(f"mode must be 'freeze' or 'unfreeze', got {mode}")
+    
+    # Get all named modules
+    all_modules = dict(root_module.named_modules())
+    
+    # Determine which modules to process
+    if not submodules:
+        # Process all modules
+        modules_to_process = all_modules.values()
+    else:
+        # Process only specified modules and their descendants
+        modules_to_process = []
+        for submodule_name in submodules:
+            if submodule_name not in all_modules:
+                raise ValueError(f"Submodule '{submodule_name}' not found in root module")
+            submodule = all_modules[submodule_name]
+            modules_to_process.append(submodule)
+            # Add all descendants
+            for name, module in all_modules.items():
+                if name.startswith(submodule_name + '.'):
+                    modules_to_process.append(module)
+    
+    # Remove duplicates while preserving order
+    modules_to_process = list(dict.fromkeys(modules_to_process))
+    
+    # Process each module
+    for module in modules_to_process:
+        # Handle parameters
+        for param in module.parameters():
+            param.requires_grad = (mode == 'unfreeze')
+        
+        # Handle batch norm running stats if requested
+        if include_bn_running_stats:
+            if mode == 'freeze':
+                # Convert BatchNorm2d/SyncBatchNorm to FrozenBatchNorm2d
+                if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):
+                    parent_name, child_name = _get_parent_and_child_name(root_module, module)
+                    if parent_name and child_name:
+                        parent_module = all_modules[parent_name]
+                        frozen_bn = _FrozenBatchNorm2d.convert_frozen_batchnorm(module)
+                        setattr(parent_module, child_name, frozen_bn)
+            else:  # unfreeze
+                # Convert FrozenBatchNorm2d back to BatchNorm2d
+                if isinstance(module, _FrozenBatchNorm2d):
+                    parent_name, child_name = _get_parent_and_child_name(root_module, module)
+                    if parent_name and child_name:
+                        parent_module = all_modules[parent_name]
+                        regular_bn = unfreeze_batch_norm_2d(module)
+                        setattr(parent_module, child_name, regular_bn)
+
+
+def freeze(root_module, submodules=[], include_bn_running_stats=True):
+    """
+    Freeze parameters of specified modules and their hierarchical descendants.
+    
+    Args:
+        root_module (nn.Module): Root module relative to which submodules are referenced.
+        submodules (list[str]): List of module names to freeze. Empty list freezes entire module.
+        include_bn_running_stats (bool): Whether to freeze batch norm running stats.
+    
+    Examples:
+        >>> model = timm.create_model('resnet18')
+        >>> # Freeze up to and including layer2
+        >>> submodules = [n for n, _ in model.named_children()]
+        >>> print(submodules)
+        ['conv1', 'bn1', 'act1', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'global_pool', 'fc']
+        >>> freeze(model, submodules[:submodules.index('layer2') + 1])
+        >>> # Check for yourself that it works as expected
+        >>> print(model.layer2[0].conv1.weight.requires_grad)
+        False
+        >>> print(model.layer3[0].conv1.weight.requires_grad)
+        True
+        >>> # Unfreeze
+        >>> unfreeze(model)
+    """
+    _freeze_unfreeze(
+        root_module=root_module,
+        submodules=submodules,
+        include_bn_running_stats=include_bn_running_stats,
+        mode='freeze'
+    )
+
+
+def unfreeze(root_module, submodules=[], include_bn_running_stats=True):
+    """
+    Unfreeze parameters of specified modules and their hierarchical descendants.
+    
+    Args:
+        root_module (nn.Module): Root module relative to which submodules are referenced.
+        submodules (list[str]): List of module names to unfreeze. Empty list unfreezes entire module.
+        include_bn_running_stats (bool): Whether to unfreeze batch norm running stats.
+    
+    See example in docstring for freeze.
+    """
+    _freeze_unfreeze(
+        root_module=root_module,
+        submodules=submodules,
+        include_bn_running_stats=include_bn_running_stats,
+        mode='unfreeze'
+    )