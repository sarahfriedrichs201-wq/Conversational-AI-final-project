# CLI Argument Parsing Testing Implementation Guide

## Overview

This document provides a detailed implementation plan for adding comprehensive testing to the CLI argument parsing functionality in the lm-evaluation-harness. The approach involves refactoring the argument parser setup and adding unit tests to ensure robust CLI argument handling.

## Current Architecture Analysis

The current CLI evaluation system uses `argparse.ArgumentParser` for command-line argument parsing. The main entry point is through `lm_eval/__main__.py` which contains the argument parsing logic.

## Implementation Plan

### Phase 1: Parser Refactoring

#### Step 1: Extract Parser Setup Logic

**File:** `lm_eval/__main__.py`

**Current State:** The `parse_eval_args` function likely contains both parser creation and argument parsing logic.

**Implementation:**

```python
def setup_parser() -> argparse.ArgumentParser:
    """
    Creates and configures the argument parser for CLI evaluation.
    Returns a configured ArgumentParser instance.
    """
    parser = argparse.ArgumentParser(
        description="Language Model Evaluation Harness",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # Add all existing argument definitions here
    # Model arguments
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="Name of model to use"
    )
    
    parser.add_argument(
        "--model_args",
        type=str,
        default="",
        help="Comma separated string of model arguments"
    )
    
    # Task arguments
    parser.add_argument(
        "--tasks",
        type=str,
        default=None,
        help="Tasks to evaluate. Use comma separation for multiple tasks"
    )
    
    # Output arguments
    parser.add_argument(
        "--output_path",
        type=str,
        default=None,
        help="Path to output results"
    )
    
    # Add all other existing arguments here...
    # Include all current argument definitions from parse_eval_args
    
    return parser

def parse_eval_args() -> argparse.Namespace:
    """
    Parse command line arguments for evaluation.
    """
    parser = setup_parser()
    args = parser.parse_args()
    return args
```

### Phase 2: Type Checking Utility

#### Step 2: Implement Argument Type Validation

**File:** `lm_eval/__main__.py`

**Implementation:**

```python
def check_argument_types(parser: argparse.ArgumentParser) -> None:
    """
    Validates that all CLI arguments have explicit type annotations.
    Raises TypeError if any argument lacks a type specification.
    
    Args:
        parser: Configured ArgumentParser instance
        
    Raises:
        TypeError: If any argument is missing type specification
    """
    missing_types = []
    
    for action in parser._actions:
        # Skip help and version actions
        if action.dest in ['help', 'version']:
            continue
            
        # Check if action has no type specified
        if action.type is None:
            missing_types.append(action.dest)
    
    if missing_types:
        raise TypeError(
            f"The following arguments are missing type specifications: {', '.join(missing_types)}. "
            "All CLI arguments must have explicit type annotations."
        )
```

### Phase 3: Enhanced CLI Evaluation with Type Checking

#### Step 3: Add Type Validation to CLI Entry Point

**File:** `lm_eval/__main__.py`

**Implementation:**

```python
def cli_evaluate() -> None:
    """
    Main CLI evaluation entry point with enhanced type checking.
    """
    try:
        # Setup and validate parser
        parser = setup_parser()
        check_argument_types(parser)
        
        # Parse arguments
        args = parser.parse_args()
        
        # Validate critical argument types
        if not isinstance(args.model, str):
            raise TypeError(f"Expected model to be str, got {type(args.model)}")
            
        if args.tasks is not None and not isinstance(args.tasks, str):
            raise TypeError(f"Expected tasks to be str or None, got {type(args.tasks)}")
            
        if args.output_path is not None and not isinstance(args.output_path, str):
            raise TypeError(f"Expected output_path to be str or None, got {type(args.output_path)}")
            
        # Add type checks for other critical arguments...
        # Continue with existing evaluation logic
        
    except TypeError as e:
        print(f"TypeError in CLI arguments: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"Error during evaluation: {e}")
        sys.exit(1)
```

### Phase 4: Unit Test Implementation

#### Step 4: Create Comprehensive Test Suite

**File:** `tests/test_cli_parsing.py`

**Implementation:**

```python
import pytest
import argparse
import sys
from unittest.mock import patch
from lm_eval.__main__ import setup_parser, check_argument_types, cli_evaluate

class TestCLIArgumentParsing:
    """Test suite for CLI argument parsing functionality."""
    
    def test_setup_parser_returns_parser(self):
        """Test that setup_parser returns an ArgumentParser instance."""
        parser = setup_parser()
        assert isinstance(parser, argparse.ArgumentParser)
    
    def test_parser_has_required_arguments(self):
        """Test that parser includes all required arguments."""
        parser = setup_parser()
        
        # Check for essential arguments
        assert hasattr(parser, '_actions')
        action_dests = [action.dest for action in parser._actions]
        
        required_args = ['model', 'tasks', 'output_path', 'model_args']
        for arg in required_args:
            assert arg in action_dests, f"Missing required argument: {arg}"
    
    def test_check_argument_types_passes_with_typed_args(self):
        """Test that type checking passes when all arguments have types."""
        parser = setup_parser()
        
        # Should not raise an exception
        check_argument_types(parser)
    
    def test_check_argument_types_fails_with_untyped_args(self):
        """Test that type checking fails when arguments lack types."""
        parser = argparse.ArgumentParser()
        parser.add_argument("--untyped_arg", help="Argument without type")
        
        with pytest.raises(TypeError, match="missing type specifications"):
            check_argument_types(parser)
    
    def test_model_argument_parsing(self):
        """Test parsing of model argument."""
        parser = setup_parser()
        
        test_args = ["--model", "gpt2", "--tasks", "hellaswag"]
        args = parser.parse_args(test_args)
        
        assert args.model == "gpt2"
        assert isinstance(args.model, str)
    
    def test_tasks_argument_parsing(self):
        """Test parsing of tasks argument with multiple values."""
        parser = setup_parser()
        
        test_args = ["--model", "gpt2", "--tasks", "hellaswag,arc_easy"]
        args = parser.parse_args(test_args)
        
        assert args.tasks == "hellaswag,arc_easy"
        assert isinstance(args.tasks, str)
    
    def test_model_args_parsing(self):
        """Test parsing of model_args string."""
        parser = setup_parser()
        
        test_args = [
            "--model", "hf-causal", 
            "--model_args", "pretrained=gpt2,use_accelerate=True",
            "--tasks", "hellaswag"
        ]
        args = parser.parse_args(test_args)
        
        assert args.model_args == "pretrained=gpt2,use_accelerate=True"
        assert isinstance(args.model_args, str)
    
    def test_output_path_parsing(self):
        """Test parsing of output_path argument."""
        parser = setup_parser()
        
        test_args = [
            "--model", "gpt2",
            "--tasks", "hellaswag", 
            "--output_path", "/tmp/results.json"
        ]
        args = parser.parse_args(test_args)
        
        assert args.output_path == "/tmp/results.json"
        assert isinstance(args.output_path, str)
    
    @patch('sys.argv', ['eval', '--model', 'gpt2', '--tasks', 'hellaswag'])
    def test_cli_evaluate_success(self):
        """Test successful CLI evaluation execution."""
        # This test might need mocking of downstream evaluation logic
        try:
            cli_evaluate()
            # If we reach here, no exceptions were raised
            assert True
        except SystemExit as e:
            # Allow system exit for successful execution
            if e.code == 0:
                assert True
            else:
                raise
    
    @patch('sys.argv', ['eval', '--model', 123])  # Invalid type for model
    def test_cli_evaluate_type_error(self, capsys):
        """Test CLI evaluation with type errors."""
        with pytest.raises(SystemExit):
            cli_evaluate()
        
        # Check that appropriate error message was printed
        captured = capsys.readouterr()
        assert "TypeError" in captured.out

class TestArgumentTypeCompatibility:
    """Test specific argument type compatibility."""
    
    @pytest.mark.parametrize("arg_name,test_value,expected_type", [
        ("model", "gpt2", str),
        ("tasks", "hellaswag,arc_easy", str),
        ("output_path", "/path/to/results", str),
        ("model_args", "key=value,flag=True", str),
        ("num_fewshot", "5", int),
        ("batch_size", "32", str),  # batch_size might be string to allow "auto"
        ("device", "cuda:0", str),
    ])
    def test_argument_type_compatibility(self, arg_name, test_value, expected_type):
        """Test that arguments accept and return expected types."""
        parser = setup_parser()
        
        # Construct test arguments
        test_args = ["--model", "dummy"]  # Required arg
        if arg_name != "model":  # Avoid duplicating model arg
            test_args.extend([f"--{arg_name}", test_value])
        
        args = parser.parse_args(test_args)
        arg_value = getattr(args, arg_name)
        
        assert isinstance(arg_value, expected_type), \
            f"Argument {arg_name} should be {expected_type}, got {type(arg_value)}"
```

### Phase 5: Integration and Validation

#### Step 5: Update Main Entry Point

**File:** `lm_eval/__main__.py`

**Implementation:**

```python
def main():
    """
    Main entry point for the lm-evaluation-harness CLI.
    """
    if len(sys.argv) == 1:
        # No arguments provided, show help
        parser = setup_parser()
        parser.print_help()
        return
    
    # Run the enhanced CLI evaluation
    cli_evaluate()

if __name__ == "__main__":
    main()
```

## Testing Strategy

### 1. Unit Tests
- Test individual parser functions in isolation
- Verify type checking functionality
- Test argument parsing with various input types

### 2. Integration Tests
- Test the full CLI workflow
- Verify error handling for malformed inputs
- Ensure backward compatibility

### 3. Edge Cases to Test
- Missing required arguments
- Invalid argument types
- Empty string values
- Special characters in file paths
- Comma-separated lists with various formats

## Code Style Guidelines

- Follow existing code formatting (PEP 8)
- Use type hints for all function signatures
- Include comprehensive docstrings
- Maintain consistent error handling patterns
- Use descriptive variable names matching existing codebase

## Validation Checklist

- [ ] `setup_parser()` returns a properly configured ArgumentParser
- [ ] `check_argument_types()` correctly identifies untyped arguments
- [ ] All existing CLI arguments have explicit type annotations
- [ ] Unit tests cover all major argument types and edge cases
- [ ] CLI evaluation handles type errors gracefully
- [ ] Backward compatibility is maintained
- [ ] Error messages are clear and actionable

## Next Steps

After implementing this testing framework, consider:

1. Adding tests for argument validation logic
2. Testing argument interdependencies
3. Adding performance tests for argument parsing
4. Extending type checking to validate argument values (not just types)
5. Adding integration tests with actual model evaluation

This implementation provides a robust foundation for testing CLI argument parsing while maintaining the existing functionality and code style of the lm-evaluation-harness project.