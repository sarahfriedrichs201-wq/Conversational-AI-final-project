diff --git a/src/nlp/arrow_dataset.py b/src/nlp/arrow_dataset.py
--- a/src/nlp/arrow_dataset.py
+++ b/src/nlp/arrow_dataset.py
@@ -304,3 +304,96 @@
             split=self.split,
             fingerprint=new_fingerprint,
         )
+
+    def filter(
+        self,
+        function: Callable,
+        with_indices: bool = False,
+        batch_size: Optional[int] = 1000,
+        remove_columns: Optional[List[str]] = None,
+        keep_in_memory: bool = False,
+        load_from_cache_file: bool = True,
+        cache_file_name: Optional[str] = None,
+        writer_batch_size: int = 1000,
+        disable_nullable: bool = True,
+        **kwargs,
+    ) -> "Dataset":
+        """Apply a filter function to all the elements in the table in batches
+        and update the table so that the dataset only includes examples according to the filter function.
+
+        Args:
+            `function` (`callable`): with one of the following signature:
+                - `function(example: Dict) -> bool` if `with_indices=False`
+                - `function(example: Dict, indices: int) -> bool` if `with_indices=True`
+            `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.
+            `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`
+                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`
+            `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.
+                Columns will be removed after filtering, i.e. if `function` is adding
+                columns with names in `remove_columns`, these columns will be kept.
+            `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.
+            `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`
+                can be identified, use it instead of recomputing.
+            `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the
+                results of the computation instead of the automatically generated cache file name.
+            `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.
+                Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.
+            `disable_nullable` (`bool`, default: `True`): Allow null values in the table.
+        """
+        if not callable(function):
+            raise TypeError("`function` must be a callable.")
+
+        new_fingerprint = generate_fingerprint(
+            self.fingerprint,
+            function=function,
+            with_indices=with_indices,
+            batch_size=batch_size,
+            remove_columns=remove_columns,
+            disable_nullable=disable_nullable,
+            **kwargs,
+        )
+
+        if cache_file_name is None:
+            cache_file_name = os.path.join(
+                get_temporary_cache_files_directory(), new_fingerprint + ".arrow"
+            )
+
+        if load_from_cache_file and os.path.exists(cache_file_name):
+            try:
+                logger.info(f"Loading cached filtered dataset from {cache_file_name}")
+                return Dataset.from_file(
+                    cache_file_name,
+                    info=self.info.copy(),
+                    split=self.split,
+                    fingerprint=new_fingerprint,
+                )
+            except Exception as e:
+                logger.warning(
+                    f"Failed to load cached filtered dataset from {cache_file_name} due to {e}. Recomputing."
+                )
+
+        def _filter_map_function(batch: Dict[str, List[Any]], indices: List[int]) -> Dict[str, List[Any]]:
+            num_examples_in_batch = len(next(iter(batch.values())))
+            
+            kept_indices_in_batch = []
+            
+            if with_indices:
+                for i in range(num_examples_in_batch):
+                    example = {col: batch[col][i] for col in batch}
+                    if function(example, indices[i], **kwargs):
+                        kept_indices_in_batch.append(i)
+            else:
+                for i in range(num_examples_in_batch):
+                    example = {col: batch[col][i] for col in batch}
+                    if function(example, **kwargs):
+                        kept_indices_in_batch.append(i)
+            
+            filtered_batch = {col: [batch[col][i] for i in kept_indices_in_batch] for col in batch}
+            return filtered_batch
+
+        return self._map_with_cache(
+            _filter_map_function,
+            with_indices=True,
+            batched=True,
+            batch_size=batch_size,
+            remove_columns=remove_columns,
+            keep_in_memory=keep_in_memory,
+            cache_file_name=cache_file_name,
+            writer_batch_size=writer_batch_size,
+            disable_nullable=disable_nullable,
+            fingerprint=new_fingerprint,
+        )