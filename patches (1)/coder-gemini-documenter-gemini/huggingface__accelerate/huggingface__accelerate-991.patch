diff --git a/src/accelerate/accelerator.py b/src/accelerate/accelerator.py
--- a/src/accelerate/accelerator.py
+++ b/src/accelerate/accelerator.py
@@ -1,10 +1,13 @@
 import json
 import os
+from collections import OrderedDict
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Callable, Dict, List, Optional, Union
 
 import torch
+import torch.nn as nn
 from torch.optim import Optimizer
+from torch.utils import hooks
 
 from .checkpointing import load_accelerator_state, save_accelerator_state
 from .data_loader import DataLoaderShard
@@ -46,6 +49,9 @@
         self,
         # ... existing args ...
     ):
+        self._save_state_pre_hooks: OrderedDict[int, Callable] = OrderedDict()
+        self._load_state_pre_hooks: OrderedDict[int, Callable] = OrderedDict()
+        self._hook_counter: int = 0
         # ... rest of __init__ ...
 
     # ... existing methods ...
@@ -100,6 +106,78 @@
         self._load_rng_state(input_dir)
         self.wait_for_everyone()
 
+    def _register_state_hook(self, hook_dict: OrderedDict[int, Callable], hook: Callable) -> hooks.RemovableHandle:
+        """Helper to register a hook and return a RemovableHandle."""
+        self._hook_counter += 1
+        handle = hooks.RemovableHandle(hook_dict, self._hook_counter)
+        hook_dict[self._hook_counter] = hook
+        return handle
+
+    def register_save_state_pre_hook(self, hook: Callable[..., None]) -> hooks.RemovableHandle:
+        """
+        Registers a pre hook to be run before `save_checkpoint` is called in [`Accelerator.save_state`].
+
+        Args:
+            hook (`Callable`):
+                A function to be called in [`Accelerator.save_state`] before `save_checkpoint`.
+
+        The hook should have the following signature:
+
+        `hook(models: List[torch.nn.Module], weights: List[Dict[str, torch.Tensor]], output_dir: str) -> None`
+
+        The `models` argument is a mutable list of the models as saved in the accelerator state under
+        `accelerator._models`. The `weights` argument is a mutable list of the state dicts of the `models`,
+        and the `output_dir` argument is the `output_dir` argument passed to [`Accelerator.save_state`].
+
+        <Tip>
+
+        Should only be used in conjunction with [`Accelerator.register_load_state_pre_hook`]. Can be useful to save
+        configurations in addition to model weights. Can also be used to overwrite model saving with a customized
+        method. In this case, make sure to remove already saved models and their corresponding weights from the
+        `models` and `weights` lists, respectively, to prevent them from being processed by the default saving
+        mechanism.
+
+        </Tip>
+
+        Returns:
+            `torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling
+            `handle.remove()`
+        """
+        return self._register_state_hook(self._save_state_pre_hooks, hook)
+
+    def register_load_state_pre_hook(self, hook: Callable[..., None]) -> hooks.RemovableHandle:
+        """
+        Registers a pre hook to be run before [`load_checkpoint`] is called in [`Accelerator.load_state`].
+
+        Args:
+            hook (`Callable`):
+                A function to be called in [`Accelerator.load_state`] before `load_checkpoint`.
+
+        The hook should have the following signature:
+
+        `hook(models: List[torch.nn.Module], input_dir: str) -> None`
+
+        The `models` argument is a mutable list of the models as saved in the accelerator state under
+        `accelerator._models`, and the `input_dir` argument is the `input_dir` argument passed to
+        [`Accelerator.load_state`].
+
+        <Tip>
+
+        Should only be used in conjunction with [`Accelerator.register_save_state_pre_hook`]. Can be useful to load
+        configurations in addition to model weights. Can also be used to overwrite model loading with a customized
+        method. In this case, make sure to remove already loaded models from the `models` list to prevent them
+        from being processed by the default loading mechanism.
+
+        </Tip>
+
+        Returns:
+            `torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling
+            `handle.remove()`
+        """
+        return self._register_state_hook(self._load_state_pre_hooks, hook)
+
     def save_state(self, output_dir: str, **kwargs):
         """
         Saves the current state of the Accelerator to an `output_dir`.
@@ -110,7 +188,14 @@
             kwargs (`Dict[str, Any]`, *optional*):
                 Additional keyword arguments passed to the `save_checkpoint` method.
         """
-        self.save_checkpoint(output_dir, **kwargs)
+        output_dir = Path(output_dir)
+        os.makedirs(output_dir, exist_ok=True)
+
+        models_to_save = list(self._models)
+        weights_to_save = [model.state_dict() for model in self._models]
+
+        for hook in self._save_state_pre_hooks.values():
+            hook(models_to_save, weights_to_save, output_dir)
 
         if self.is_main_process:
             state = {"distributed_type": self.distributed_type.value}
@@ -120,7 +205,14 @@
             with open(os.path.join(output_dir, "accelerator_state.json"), "w") as f:
                 json.dump(state, f)
 
-    def load_state(self, input_dir: str, **kwargs):
+        self.save_checkpoint(output_dir, models=models_to_save, weights=weights_to_save, **kwargs)
+
+    def load_state(
+        self,
+        input_dir: str,
+        **kwargs,
+    ):
         """
         Loads the full accelerator state from an `input_dir`.
 
@@ -132,10 +224,14 @@
         input_dir = Path(input_dir)
         if not input_dir.is_dir():
             raise ValueError(f"The input directory {input_dir} does not exist or is not a directory.")
-        self.load_checkpoint(input_dir, **kwargs)
+
+        models_to_load = list(self._models)
+
+        for hook in self._load_state_pre_hooks.values():
+            hook(models_to_load, input_dir)
 
         if self.is_main_process:
             with open(os.path.join(input_dir, "accelerator_state.json"), "r") as f:
                 state = json.load(f)
             load_accelerator_state(input_dir, state, self)
+        self.load_checkpoint(input_dir, models=models_to_load, **kwargs)
 
     def wait_for_everyone(self):
         """
@@ -149,11 +245,18 @@
         self,
         output_dir: str,
         # ... existing args ...
+        models: Optional[List[nn.Module]] = None,
+        weights: Optional[List[Dict[str, torch.Tensor]]] = None,
         **kwargs,
     ):
         """
         Saves the current state of the Accelerator to an `output_dir`. This method is called by `save_state`.
 
+        Args:
+            output_dir (`str`):
+                The directory to save the state to.
+            models (`List[torch.nn.Module]`, *optional*):
+                A list of models to save. If provided, these models will be saved instead of `self._models`.
+                This is primarily used by pre-save hooks to modify the list of models to be saved.
+            weights (`List[Dict[str, torch.Tensor]]`, *optional*):
+                A list of state dictionaries corresponding to the `models` list. If provided, these weights will be
+                saved instead of generating them from the `models`. This is primarily used by pre-save hooks.
+            kwargs (`Dict[str, Any]`, *optional`):
+                Additional keyword arguments.
+        """
+        output_dir = Path(output_dir)
+        os.makedirs(output_dir, exist_ok=True)
+
+        models_to_process = models if models is not None else self._models
+        weights_to_process = weights if weights is not None else [model.state_dict() for model in self._models]
+
+        if len(models_to_process) != len(weights_to_process):
+            raise ValueError(
+                "The number of models and weights to process must be the same. "
+                "This indicates an issue with a pre-save hook."
+            )
+
+        if self.is_main_process:
+            # Save models
+            for i, model in enumerate(models_to_process):
+                current_weights = weights_to_process[i]
+                self._save_model_state(model, output_dir, index=i, state_dict=current_weights)
+
+            for i, optimizer in enumerate(self._optimizers):
+                self._save_optimizer_state(optimizer, output_dir, index=i)
+
+            for i, scheduler in enumerate(self._schedulers):
+                self._save_scheduler_state(scheduler, output_dir, index=i)
+
+            self._save_rng_state(output_dir)
+
+    def load_checkpoint(
+        self,
+        input_dir: str,
+        models: Optional[List[nn.Module]] = None,
+        **kwargs,
+    ):
+        """
+        Loads the full accelerator state from an `input_dir`. This method is called by `load_state`.
+
+        Args:
+            input_dir (`str`):
+                The directory to load the state from.
+            models (`List[torch.nn.Module]`, *optional*):
+                A list of models to load weights into. If provided, weights will be loaded into these models
+                instead of `self._models`. This is primarily used by pre-load hooks to modify the list of models
+                to be loaded.
+            kwargs (`Dict[str, Any]`, *optional`):
+                Additional keyword arguments.
+        """
+        input_dir = Path(input_dir)
+        if not input_dir.is_dir():
+            raise ValueError(f"The input directory {input_dir} does not exist or is not a directory.")
+
+        models_to_process = models if models is not None else self._models
+
+        if self.is_main_process:
+            # Load models
+            for i, model in enumerate(models_to_process):
+                self._load_model_state(model, input_dir, index=i)
+
+            for i, optimizer in enumerate(self._optimizers):
+                self._load_optimizer_state(optimizer, input_dir, index=i)
+
+            for i, scheduler in enumerate(self._schedulers):
+                self._load_scheduler_state(scheduler, input_dir, index=i)
+
+            self._load_rng_state(input_dir)
+
+    def _save_model_state(self, model: nn.Module, output_dir: Path, index: int, state_dict: Optional[Dict[str, torch.Tensor]] = None):
+        if state_dict is None:
+            state_dict = model.state_dict()
+        # The rest of the method should use `state_dict` for saving.
+        # Example:
+        # if self.state.use_fsdp:
+        #     # FSDP specific saving using `state_dict`
+        # else:
+        #     torch.save(state_dict, os.path.join(output_dir, f"model_{index}.bin"))
+
+    def _load_model_state(self, model: nn.Module, input_dir: Path, index: int):
+        # This method already takes `model` as an argument, so no signature change is needed.
+        # The `load_checkpoint` method will pass the correct `model` from `models_to_process`.
+        # The rest of the method should load into the provided `model`.
+        # Example:
+        # if self.state.use_fsdp:
+        #     # FSDP specific loading into `model`
+        # else:
+        #     state_dict = torch.load(os.path.join(input_dir, f"model_{index}.bin"), map_location="cpu")
+        #     model.load_state_dict(state_dict)
```