diff --git a/src/accelerate/accelerator.py b/src/accelerate/accelerator.py
--- a/src/accelerate/accelerator.py
+++ b/src/accelerate/accelerator.py
@@ -23,6 +23,7 @@
 from typing import List, Optional, Union
 
 import torch
+import torch.utils.hooks as hooks
 
 from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
 from .data_loader import DataLoaderDispatcher, prepare_data_loader
@@ -348,6 +349,8 @@
         self._models = []
         self._schedulers = []
         self._dataloaders = []
         self._custom_objects = []
+        self._save_state_pre_hooks = []
+        self._load_state_pre_hooks = []
 
         # RNG Types
         self.rng_types = rng_types
@@ -1070,6 +1073,12 @@
         os.makedirs(output_dir, exist_ok=True)
         logger.info(f"Saving current state to {output_dir}")
 
+        # Prepare lists for hooks and default saving
+        # `weights_for_accelerator_state` will contain state_dicts of non-distributed models
+        # that are not handled by hooks.
+        weights_for_accelerator_state = []
+        models_for_hooks = list(self._models)  # Pass a copy to avoid direct modification of self._models by hook
+
         # Save the models taking care of FSDP and DeepSpeed nuances
         weights = []
         for i, model in enumerate(self._models):
@@ -1090,7 +1099,11 @@
                 model.save_checkpoint(output_dir)
                 logger.info(f"Megatron-LM Model , Optimizer and Scheduler saved to output dir {output_dir}")
             else:
-                weights.append(self.get_state_dict(model, unwrap=False))
+                # For non-distributed models, collect their state dicts for hooks and default saving
+                weights_for_accelerator_state.append(self.get_state_dict(model, unwrap=False))
+
+        # Call pre-save hooks. Hooks are expected to modify `weights_for_accelerator_state` in place.
+        for hook in self._save_state_pre_hooks:
+            hook(models_for_hooks, weights_for_accelerator_state, output_dir)
 
         # Save the optimizers taking care of FSDP and DeepSpeed nuances
         optimizers = []
@@ -1109,7 +1122,7 @@
         elif self.distributed_type not in [DistributedType.MEGATRON_LM]:
             schedulers = self._schedulers
 
-        save_location = save_accelerator_state(
-            output_dir, weights, optimizers, schedulers, self.state.process_index, self.scaler
+        save_location = save_accelerator_state(  # optimizers and schedulers are not filtered by hooks in this minimal implementation
+            output_dir, weights_for_accelerator_state, optimizers, schedulers, self.state.process_index, self.scaler
         )
         for i, obj in enumerate(self._custom_objects):
             save_custom_state(obj, output_dir, i)
@@ -1130,6 +1143,13 @@
             raise ValueError(f"Tried to find {input_dir} but folder does not exist")
         logger.info(f"Loading states from {input_dir}")
 
+        # Prepare lists for hooks and default loading
+        # `models_to_load_by_default` will contain non-distributed models that are not handled by hooks.
+        models_to_load_by_default = []
+        # `models_for_hooks` is a mutable copy of `self._models` for the hook to modify.
+        models_for_hooks = list(self._models)
+        for hook in self._load_state_pre_hooks:
+            hook(models_for_hooks, input_dir)
         # Load the models taking care of FSDP and DeepSpeed nuances
         models = []
         for i, model in enumerate(self._models):
@@ -1147,7 +1167,8 @@
                 model.load_checkpoint(input_dir)
                 logger.info(f"Megatron-LM Model , Optimizer and Scheduler loaded from input dir {input_dir}")
             else:
-                models.append(model)
+                if model in models_for_hooks:  # Check if the hook removed this model
+                    models_to_load_by_default.append(model)
 
         # Load the optimizers taking care of FSDP and DeepSpeed nuances
         optimizers = []
@@ -1166,7 +1187,7 @@
         elif self.distributed_type not in [DistributedType.MEGATRON_LM]:
             schedulers = self._schedulers
 
-        load_accelerator_state(
-            input_dir, models, optimizers, schedulers, self.state.process_index, self.scaler, **load_model_func_kwargs
+        load_accelerator_state(  # optimizers and schedulers are not filtered by hooks in this minimal implementation
+            input_dir, models_to_load_by_default, optimizers, schedulers, self.state.process_index, self.scaler, **load_model_func_kwargs
         )
         custom_checkpoints = [f for f in os.listdir(input_dir) if "custom_checkpoint" in f]
         if len(custom_checkpoints) != len(self._custom_objects):
@@ -1265,3 +1286,59 @@
             if optimizer.step_was_skipped:
                 return True
         return False
+
+    def register_save_state_pre_hook(self, hook: Callable[..., None]) -> hooks.RemovableHandle:
+        """
+        Registers a pre hook to be run before `save_checkpoint` is called in [`Accelerator.save_state`].
+
+        Args:
+            hook (`Callable`):
+                A function to be called in [`Accelerator.save_state`] before `save_checkpoint`.
+
+        The hook should have the following signature:
+
+        `hook(models: List[torch.nn.Module], weights: List[Dict[str, torch.Tensor]], output_dir: str) -> None`
+
+        The `models` argument are the models as saved in the accelerator state under `accelerator._models`, `weights`
+        argument are the state dicts of the non-distributed `models` (this list can be modified by the hook), and the
+        `output_dir` argument is the `output_dir` argument passed to [`Accelerator.save_state`].
+
+        <Tip>
+
+        Should only be used in conjunction with [`Accelerator.register_load_state_pre_hook`]. Can be useful to save
+        configurations in addition to model weights. Can also be used to overwrite model saving with a customized
+        method. In this case, make sure to remove already loaded weights from the weights list.
+
+        </Tip>
+
+        Returns:
+            `torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling
+            `handle.remove()`
+        """
+        handle = hooks.RemovableHandle(self._save_state_pre_hooks)
+        self._save_state_pre_hooks.append(hook)
+        return handle
+
+    def register_load_state_pre_hook(self, hook: Callable[..., None]) -> hooks.RemovableHandle:
+        """
+        Registers a pre hook to be run before [`load_checkpoint`] is called in [`Accelerator.load_state`].
+
+        Args:
+            hook (`Callable`):
+                A function to be called in [`Accelerator.load_state`] before `load_checkpoint`.
+
+        The hook should have the following signature:
+
+        `hook(models: List[torch.nn.Module], input_dir: str) -> None`
+
+        The `models` argument is a mutable list of models as saved in the accelerator state under `accelerator._models`
+        (this list can be modified by the hook), and the `input_dir` argument is the `input_dir` argument passed to
+        [`Accelerator.load_state`].
+
+        <Tip>
+
+        Should only be used in conjunction with [`Accelerator.register_save_state_pre_hook`]. Can be useful to load
+        configurations in addition to model weights. Can also be used to overwrite model loading with a customized
+        method. In this case, make sure to remove already loaded models from the models list.
+
+        </Tip>
+
+        Returns:
+            `torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling
+            `handle.remove()`
+        """
+        handle = hooks.RemovableHandle(self._load_state_pre_hooks)
+        self._load_state_pre_hooks.append(hook)
+        return handle