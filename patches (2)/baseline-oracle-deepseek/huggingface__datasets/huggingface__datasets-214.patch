diff --git a/src/nlp/arrow_dataset.py b/src/nlp/arrow_dataset.py
--- a/src/nlp/arrow_dataset.py
+++ b/src/nlp/arrow_dataset.py
@@ -1,5 +1,5 @@
 # coding=utf-8
-# Copyright 2020 The HuggingFace Authors.
+# Copyright 2020 The HuggingFace Authors, 2021 The HuggingFace Datasets Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -17,6 +17,7 @@
 import logging
 import os
 from collections.abc import Mapping
+from functools import partial
 from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
@@ -24,7 +25,7 @@
 from tqdm import tqdm
 
 from nlp.utils.py_utils import dumps
-
+from .utils import convert_tuples_in_lists
 from .arrow_writer import ArrowWriter
 from .utils import convert_tuples_in_lists
 
@@ -401,6 +402,7 @@
         writer_batch_size: Optional[int] = 1000,
         arrow_schema: Optional[pa.Schema] = None,
         disable_nullable: bool = True,
+        **kwargs,
     ):
         """ Apply a function to all the elements in the table (individually or in batches)
             and update the table (if function does updated examples).
@@ -548,6 +550,157 @@
         else:
             return self
 
+    def filter(
+        self,
+        function,
+        with_indices: bool = False,
+        batch_size: Optional[int] = 1000,
+        remove_columns: Optional[List[str]] = None,
+        keep_in_memory: bool = False,
+        load_from_cache_file: bool = True,
+        cache_file_name: Optional[str] = None,
+        writer_batch_size: Optional[int] = 1000,
+        disable_nullable: bool = True,
+        **kwargs,
+    ):
+        """ Apply a filter function to all the elements in the table in batches
+        and update the table so that the dataset only includes examples according to the filter function.
+
+        Args:
+            `function` (`callable`): with one of the following signature:
+                - `function(example: Dict) -> bool` if `with_indices=False`
+                - `function(example: Dict, indices: int) -> bool` if `with_indices=True`
+            `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.
+            `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`
+                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`
+            `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.
+                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding
+                columns with names in `remove_columns`, these columns will be kept.
+            `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.
+            `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`
+                can be identified, use it instead of recomputing.
+            `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the
+                results of the computation instead of the automatically generated cache file name.
+            `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.
+                Higher value gives smaller cache files, lower value consume less temporary memory while running `.filter()`.
+            `disable_nullable` (`bool`, default: `True`): Allow null values in the table.
+        """
+        # If the array is empty we do nothing
+        if len(self) == 0:
+            return self
+
+        # Select the columns (arrow columns) to process
+        if remove_columns is not None and any(col not in self._data.column_names for col in remove_columns):
+            raise ValueError(
+                "Column to remove {} not in the dataset. Current columns in the dataset: {}".format(
+                    list(filter(lambda col: col not in self._data.column_names, remove_columns)),
+                    self._data.column_names,
+                )
+            )
+
+        # If we do batch computation but no batch size is provided, default to the full dataset
+        batched = True
+        if batch_size is None or batch_size <= 0:
+            batch_size = self._data.num_rows
+
+        # Check if the function returns bool
+        def does_function_return_bool(inputs, indices):
+            """ Does the function returns a bool. """
+            processed_inputs = function(inputs, indices) if with_indices else function(inputs)
+            does_return_bool = isinstance(processed_inputs, bool)
+
+            if does_return_bool is False:
+                raise TypeError(
+                    "Provided `function` which is applied to all elements of table returns a variable of type {}. Make sure provided `function` returns a variable of type `bool`.".format(
+                        type(processed_inputs)
+                    )
+                )
+            return does_return_bool
+
+        # Test it on the first element or a small batch (0, 1) for batched inputs
+        test_inputs = self[:2] if batched else self[0]
+        test_indices = [0, 1] if batched else 0
+        does_function_return_bool(test_inputs, test_indices)
+
+        # Define a mapping function that filters examples
+        def map_function_for_filter(batch, indices):
+            """ Utility to apply the filter function on a batch and return only the examples that pass. """
+            # Apply filter
+            if with_indices:
+                filter_results = function(batch, indices)
+            else:
+                filter_results = function(batch)
+            # Convert to list if single bool (non-batched case handled by batched=True)
+            if isinstance(filter_results, bool):
+                filter_results = [filter_results]
+            # Keep only examples where filter is True
+            filtered_batch = {}
+            for key in batch:
+                filtered_batch[key] = [value for i, value in enumerate(batch[key]) if filter_results[i]]
+            return filtered_batch
+
+        # Find the output schema if none is given
+        test_inputs = self[:2]
+        test_indices = [0, 1]
+        test_output = map_function_for_filter(test_inputs, test_indices)
+        arrow_schema = pa.Table.from_pydict(test_output).schema
+        if disable_nullable:
+            arrow_schema = pa.schema(pa.field(field.name, field.type, nullable=False) for field in arrow_schema)
+
+        # Check if we've already cached this computation (indexed by a hash)
+        if self._data_files:
+            if cache_file_name is None:
+                # we create a unique hash from the function, current dataset file and the filter args
+                cache_kwargs = {
+                    "with_indices": with_indices,
+                    "batch_size": batch_size,
+                    "remove_columns": remove_columns,
+                    "keep_in_memory": keep_in_memory,
+                    "load_from_cache_file": load_from_cache_file,
+                    "cache_file_name": cache_file_name,
+                    "writer_batch_size": writer_batch_size,
+                    "disable_nullable": disable_nullable,
+                }
+                cache_file_name = self._get_cache_file_path(function, cache_kwargs)
+            if os.path.exists(cache_file_name) and load_from_cache_file:
+                logger.info("Loading cached filtered dataset at %s", cache_file_name)
+                return Dataset.from_file(cache_file_name)
+
+        # Prepare output buffer and batched writer in memory or on file
+        if keep_in_memory or not self._data_files:
+            buf_writer = pa.BufferOutputStream()
+            writer = ArrowWriter(schema=arrow_schema, stream=buf_writer, writer_batch_size=writer_batch_size)
+        else:
+            buf_writer = None
+            logger.info("Caching filtered dataset at %s", cache_file_name)
+            writer = ArrowWriter(schema=arrow_schema, path=cache_file_name, writer_batch_size=writer_batch_size)
+
+        # Loop over batches and write filtered examples to buffer/file
+        for i in tqdm(range(0, len(self), batch_size)):
+            batch = self[i : i + batch_size]
+            indices = list(range(*(slice(i, i + batch_size).indices(self._data.num_rows))))
+            filtered_batch = map_function_for_filter(batch, indices)
+            # Write batch if there are any examples left after filtering
+            if filtered_batch and len(filtered_batch[list(filtered_batch.keys())[0]]) > 0:
+                writer.write_batch(filtered_batch)
+
+        writer.finalize()
+
+        # Create new Dataset from buffer or file
+        if buf_writer is None:
+            return Dataset.from_file(cache_file_name)
+        else:
+            return Dataset.from_buffer(buf_writer.getvalue())
+
+    def _get_cache_file_path(self, function, cache_kwargs):
+        """ Find a unique name from the filenames, kwargs and the function """
+        if not self._data_files or "filename" not in self._data_files[0]:
+            return None
+        previous_files_string = "-".join(
+            "-".join(str(k) + "-" + str(v) for k, v in f.items()) for f in self._data_files
+        )
+        cache_kwargs_string = "-".join(str(k) + "-" + str(v) for k, v in cache_kwargs.items())
+        function_bytes = dumps(function)
+        output_hash = hashlib.md5(
+            previous_files_string.encode("utf-8") + cache_kwargs_string.encode("utf-8") + function_bytes
+        ).hexdigest()
+        cache_file_name = "cache-" + output_hash + ".arrow"
+        cache_directory = os.path.dirname(self._data_files[0]["filename"])
+        cache_file_path = os.path.join(cache_directory, cache_file_name)
+        return cache_file_path
