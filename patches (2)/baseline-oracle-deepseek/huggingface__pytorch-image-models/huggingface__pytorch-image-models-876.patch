diff --git a/timm/utils/model.py b/timm/utils/model.py
--- a/timm/utils/model.py
+++ b/timm/utils/model.py
@@ -1,4 +1,5 @@
 """ Model / state_dict utils
+Hacked together by / Copyright 2020 Ross Wightman
 """
 from .model_ema import ModelEma
 import torch 
@@ -77,3 +78,154 @@ def extract_spp_stats(model, 
     hook = ActivationStatsHook(model, hook_fn_locs=hook_fn_locs, hook_fns=hook_fns)
     _ = model(x)
     return hook.stats
+
+
+def freeze_batch_norm_2d(module):
+    """Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`.
+    If `module` is itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into
+    `FrozenBatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules are
+    converted in place.
+
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+
+    Returns:
+        torch.nn.Module: Resulting module
+
+    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
+    """
+    import torch.nn as nn
+    from timm.layers import FrozenBatchNorm2d
+
+    if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):
+        frozen_bn = FrozenBatchNorm2d(
+            num_features=module.num_features,
+            eps=module.eps,
+            momentum=module.momentum,
+        )
+        if module.affine:
+            frozen_bn.weight.data.copy_(module.weight.data)
+            frozen_bn.bias.data.copy_(module.bias.data)
+        frozen_bn.running_mean.data.copy_(module.running_mean.data)
+        frozen_bn.running_var.data.copy_(module.running_var.data)
+        frozen_bn.num_batches_tracked.data.copy_(module.num_batches_tracked.data)
+        return frozen_bn
+
+    for name, child in module.named_children():
+        new_child = freeze_batch_norm_2d(child)
+        if new_child is not child:
+            setattr(module, name, new_child)
+    return module
+
+
+def unfreeze_batch_norm_2d(module):
+    """Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`.
+    If `module` is itself an instance of `FrozenBatchNorm2d`, it is converted into
+    `BatchNorm2d` and returned. Otherwise, the module is walked recursively and submodules are
+    converted in place.
+
+    Args:
+        module (torch.nn.Module): Any PyTorch module.
+
+    Returns:
+        torch.nn.Module: Resulting module
+
+    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762
+    """
+    import torch.nn as nn
+    from timm.layers import FrozenBatchNorm2d
+
+    if isinstance(module, FrozenBatchNorm2d):
+        bn = nn.BatchNorm2d(
+            num_features=module.num_features,
+            eps=module.eps,
+            momentum=module.momentum,
+            affine=module.weight is not None,
+        )
+        if module.weight is not None:
+            bn.weight.data.copy_(module.weight.data)
+            bn.bias.data.copy_(module.bias.data)
+        bn.running_mean.data.copy_(module.running_mean.data)
+        bn.running_var.data.copy_(module.running_var.data)
+        bn.num_batches_tracked.data.copy_(module.num_batches_tracked.data)
+        return bn
+
+    for name, child in module.named_children():
+        new_child = unfreeze_batch_norm_2d(child)
+        if new_child is not child:
+            setattr(module, name, new_child)
+    return module
+
+
+def _freeze_unfreeze(root_module, submodules=[], include_bn_running_stats=True, mode='freeze'):
+    """Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which the `submodules` are referenced.
+        submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as
+            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list
+            means that the whole root module will be (un)frozen. Defaults to [].
+        include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.
+            Defaults to `True`.
+        mode (str): Whether to freeze ("freeze") or unfreeze ("unfreeze"). Defaults to `"freeze"`.
+    """
+    import torch.nn as nn
+
+    if not submodules:
+        submodules = ['']
+
+    for name, module in root_module.named_modules():
+        # Check if this module matches any of the submodules patterns
+        matched = False
+        for pattern in submodules:
+            if fnmatch.fnmatch(name, pattern):
+                matched = True
+                break
+        if not matched:
+            continue
+
+        # Freeze/unfreeze parameters
+        for param in module.parameters():
+            param.requires_grad = (mode == 'unfreeze')
+
+        # Handle batch norm running stats
+        if include_bn_running_stats:
+            if mode == 'freeze':
+                freeze_batch_norm_2d(module)
+            else:
+                unfreeze_batch_norm_2d(module)
+
+
+def freeze(root_module, submodules=[], include_bn_running_stats=True):
+    """Freeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which `submodules` are referenced.
+        submodules (list[str]): List of modules for which the parameters will be frozen. They are to be provided as
+            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list
+            means that the whole root module will be frozen. Defaults to `[]`.
+        include_bn_running_stats (bool): Whether to also freeze the running statistics of `BatchNorm2d` and
+            `SyncBatchNorm` layers. These will be converted to `FrozenBatchNorm2d` in place. Hint: During fine tuning,
+            it's good practice to freeze batch norm stats. And note that these are different to the affine parameters
+            which are just normal PyTorch parameters. Defaults to `True`.
+
+    Hint: If you want to freeze batch norm ONLY, use `timm.utils.model.freeze_batch_norm_2d`.
+    """
+    _freeze_unfreeze(root_module, submodules, include_bn_running_stats, mode='freeze')
+
+
+def unfreeze(root_module, submodules=[], include_bn_running_stats=True):
+    """Unfreeze parameters of the specified modules and those of all their hierarchical descendants.
+    This is done in place.
+
+    Args:
+        root_module (nn.Module): Root module relative to which `submodules` are referenced.
+        submodules (list[str]): List of submodules for which the parameters will be (un)frozen. They are to be provided
+            as named modules relative to the root module (accessible via `root_module.named_modules()`). An empty
+            list means that the whole root module will be unfrozen. Defaults to `[]`.
+        include_bn_running_stats (bool): Whether to also unfreeze the running statistics of `FrozenBatchNorm2d` layers.
+            These will be converted to `BatchNorm2d` in place. Defaults to `True`.
+    """
+    _freeze_unfreeze(root_module, submodules, include_bn_running_stats, mode='unfreeze')
