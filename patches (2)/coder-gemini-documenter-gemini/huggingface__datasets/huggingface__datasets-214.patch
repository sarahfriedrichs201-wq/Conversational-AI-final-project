diff --git a/src/nlp/arrow_dataset.py b/src/nlp/arrow_dataset.py
--- a/src/nlp/arrow_dataset.py
+++ b/src/nlp/arrow_dataset.py
@@ -628,3 +628,107 @@
             if remove_columns is not None:
                 ds.drop(remove_columns)
             return ds
+
+    def filter(
+        self,
+        function: Callable,
+        with_indices: bool = False,
+        batch_size: Optional[int] = 1000,
+        remove_columns: Optional[List[str]] = None,
+        keep_in_memory: bool = False,
+        load_from_cache_file: bool = True,
+        cache_file_name: Optional[str] = None,
+        writer_batch_size: Optional[int] = 1000,
+        disable_nullable: bool = True,
+    ):
+        """ Apply a filter function to all the elements in the table in batches
+            and update the table so that the dataset only includes examples according to the filter function.
+
+            Args:
+                `function` (`callable`): with one of the following signature:
+                    - `function(example: Dict) -> bool` if `with_indices=False`
+                    - `function(example: Dict, indices: int) -> bool` if `with_indices=True`
+                `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.
+                `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`
+                    `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`
+                `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.
+                    Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding
+                    columns with names in `remove_columns`, these columns will be kept.
+                `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.
+                `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`
+                    can be identified, use it instead of recomputing.
+                `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the
+                    results of the computation instead of the automatically generated cache file name.
+                `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.
+                    Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.
+                `disable_nullable` (`bool`, default: `True`): Allow null values in the table.
+        """
+        # If the array is empty we do nothing
+        if len(self) == 0:
+            return self
+
+        # If no batch size is provided, default to the full dataset
+        if batch_size is None or batch_size <= 0:
+            batch_size = self._data.num_rows
+
+        output_schema_for_writer = self._data.schema
+        if disable_nullable:
+            output_schema_for_writer = pa.schema(pa.field(field.name, field.type, nullable=False) for field in output_schema_for_writer)
+
+        # Check if we've already cached this computation (indexed by a hash)
+        if self._data_files:
+            if cache_file_name is None:
+                # we create a unique hash from the function, current dataset file and the mapping args
+                cache_kwargs = {
+                    "with_indices": with_indices,
+                    "batch_size": batch_size,
+                    "remove_columns": remove_columns,
+                    "keep_in_memory": keep_in_memory,
+                    "load_from_cache_file": load_from_cache_file,
+                    "writer_batch_size": writer_batch_size,
+                    "disable_nullable": disable_nullable,
+                }
+                cache_file_name = self._get_cache_file_path(function, cache_kwargs)
+            if os.path.exists(cache_file_name) and load_from_cache_file:
+                logger.info("Loading cached processed dataset at %s", cache_file_name)
+                ds = Dataset.from_file(cache_file_name)
+                if remove_columns is not None:
+                    ds.drop(remove_columns)
+                return ds
+
+        # Prepare output buffer and batched writer in memory or on file
+        if keep_in_memory or not self._data_files:
+            buf_writer = pa.BufferOutputStream()
+            writer = ArrowWriter(schema=output_schema_for_writer, stream=buf_writer, writer_batch_size=writer_batch_size)
+        else:
+            buf_writer = None
+            logger.info("Caching processed dataset at %s", cache_file_name)
+            writer = ArrowWriter(schema=output_schema_for_writer, path=cache_file_name, writer_batch_size=writer_batch_size)
+
+        # Loop over batches, apply function to individual examples, and write filtered examples
+        for i in tqdm(range(0, len(self), batch_size)):
+            batch_examples = self[i : i + batch_size]  # This is a dict of lists
+            indices = list(range(*(slice(i, i + batch_size).indices(self._data.num_rows))))
+
+            # Unbatch the examples to pass to the user's function
+            unbatched_examples = [{col: batch_examples[col][k] for col in batch_examples} for k in range(len(indices))]
+
+            kept_examples_list = []
+            for j, example in enumerate(unbatched_examples):
+                idx = indices[j]
+                if with_indices:
+                    keep = function(example, idx)
+                else:
+                    keep = function(example)
+
+                if not isinstance(keep, bool):
+                    raise TypeError(
+                        "The `function` passed to `filter` must return a boolean value. "
+                        f"Got type {type(keep)} for example at index {idx}."
+                    )
+
+                if keep:
+                    kept_examples_list.append(example)
+
+            if kept_examples_list:
+                # Re-batch the kept examples
+                # Assuming all examples in kept_examples_list have the same columns
+                kept_batch = {col: [ex[col] for ex in kept_examples_list] for col in kept_examples_list[0]}
+                writer.write_batch(kept_batch)
+
+        writer.finalize()
+
+        # Create new Dataset from buffer or file
+        if buf_writer is None:
+            ds = Dataset.from_file(cache_file_name)
+        else:
+            ds = Dataset.from_buffer(buf_writer.getvalue())
+
+        if remove_columns is not None:
+            ds.drop(remove_columns)
+        return ds
