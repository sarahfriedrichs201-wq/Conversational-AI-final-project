diff --git a/src/nlp/arrow_dataset.py b/src/nlp/arrow_dataset.py
--- a/src/nlp/arrow_dataset.py
+++ b/src/nlp/arrow_dataset.py
@@ -403,3 +403,113 @@
             else:
                 return Dataset.from_buffer(buf_writer.getvalue())
         else:
             return self
+
+    def filter(
+        self,
+        function,
+        with_indices: bool = False,
+        batched: bool = False,
+        batch_size: Optional[int] = 1000,
+        keep_in_memory: bool = False,
+        load_from_cache_file: bool = True,
+        cache_file_name: Optional[str] = None,
+        writer_batch_size: Optional[int] = 1000,
+        disable_nullable: bool = True,
+    ):
+        """ Apply a filter function to all the elements in the table in batches
+            and update the table so that the dataset only includes examples according to the filter function.
+
+            Args:
+                `function` (`callable`): with one of the following signature:
+                    - `function(example: Dict) -> bool` if `batched=False` and `with_indices=False`
+                    - `function(example: Dict, indices: int) -> bool` if `batched=False` and `with_indices=True`
+                    - `function(batch: Dict[List]) -> List[bool]` if `batched=True` and `with_indices=False`
+                    - `function(batch: Dict[List], indices: List[int]) -> List[bool]` if `batched=True` and `with_indices=True`
+                `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.
+                `batched` (`bool`, default: `False`): Provide batch of examples to `function`
+                `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`
+                    `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`
+                `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.
+                `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`
+                    can be identified, use it instead of recomputing.
+                `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the
+                    results of the computation instead of the automatically generated cache file name.
+                `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.
+                    Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.
+                `disable_nullable` (`bool`, default: `True`): Allow null values in the table.
+        """
+        # If the array is empty we do nothing
+        if len(self) == 0:
+            return self
+
+        # Filter always updates the data table
+        update_data = True
+
+        # If we do batch computation but no batch size is provided, default to the full dataset
+        if batched and (batch_size is None or batch_size <= 0):
+            batch_size = self._data.num_rows
+
+        # Check if the function returns a boolean (or list of booleans for batched)
+        test_inputs = self[:2] if batched else self[0]
+        test_indices = list(range(2)) if batched else 0
+        test_output = function(test_inputs, test_indices) if with_indices else function(test_inputs)
+
+        if batched:
+            if not isinstance(test_output, list) or not all(isinstance(x, bool) for x in test_output):
+                raise TypeError(
+                    "When `batched=True`, `function` must return a list of booleans. Got: {}".format(type(test_output))
+                )
+            if len(test_output) != len(test_inputs[self.column_names[0]]):
+                raise ValueError(
+                    "The number of elements returned by `function` must match the batch size. Expected: {}, Got: {}".format(
+                        len(test_inputs[self.column_names[0]]), len(test_output)
+                    )
+                )
+        else:
+            if not isinstance(test_output, bool):
+                raise TypeError(
+                    "When `batched=False`, `function` must return a boolean. Got: {}".format(type(test_output))
+                )
+
+        # The schema of the filtered dataset is the same as the original dataset
+        arrow_schema = self._data.schema
+        if disable_nullable:
+            arrow_schema = pa.schema(pa.field(field.name, field.type, nullable=False) for field in arrow_schema)
+
+        # Check if we've already cached this computation (indexed by a hash)
+        if self._data_files and update_data:
+            if cache_file_name is None:
+                # we create a unique hash from the function, current dataset file and the mapping args
+                cache_kwargs = {
+                    "with_indices": with_indices,
+                    "batched": batched,
+                    "batch_size": batch_size,
+                    "keep_in_memory": keep_in_memory,
+                    "load_from_cache_file": load_from_cache_file,
+                    "cache_file_name": cache_file_name,
+                    "writer_batch_size": writer_batch_size,
+                    "disable_nullable": disable_nullable,
+                }
+                cache_file_name = self._get_cache_file_path(function, cache_kwargs)
+            if os.path.exists(cache_file_name) and load_from_cache_file:
+                logger.info("Loading cached processed dataset at %s", cache_file_name)
+                return Dataset.from_file(cache_file_name)
+
+        # Prepare output buffer and batched writer in memory or on file
+        if keep_in_memory or not self._data_files:
+            buf_writer = pa.BufferOutputStream()
+            writer = ArrowWriter(schema=arrow_schema, stream=buf_writer, writer_batch_size=writer_batch_size)
+        else:
+            buf_writer = None
+            logger.info("Caching processed dataset at %s", cache_file_name)
+            writer = ArrowWriter(schema=arrow_schema, path=cache_file_name, writer_batch_size=writer_batch_size)
+
+        # Loop over single examples or batches and write to buffer/file if examples pass the filter
+        if not batched:
+            for i, example in tqdm(enumerate(self)):
+                keep_example = function(example, i) if with_indices else function(example)
+                if keep_example:
+                    writer.write(example)
+        else:
+            for i in tqdm(range(0, len(self), batch_size)):
+                batch = self[i : i + batch_size]
+                indices = list(range(*(slice(i, i + batch_size).indices(self._data.num_rows))))
+                keep_batch_examples = function(batch, indices) if with_indices else function(batch)
+
+                # Filter the batch based on the boolean results
+                filtered_batch = {col: [batch[col][j] for j, keep in enumerate(keep_batch_examples) if keep] for col in batch}
+                if filtered_batch[self.column_names[0]]:  # Only write if there are examples to write
+                    writer.write_batch(filtered_batch)
+
+        writer.finalize(close_stream=bool(buf_writer is None))
+
+        # Create new Dataset from buffer or file
+        if buf_writer is None:
+            return Dataset.from_file(cache_file_name)
+        else:
+            return Dataset.from_buffer(buf_writer.getvalue())